from pyspark.sql import SparkSession
from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF
from pyspark.ml.clustering import KMeans
from pyspark.ml import Pipeline
import numpy as np
from pyspark.sql.functions import col

# 1. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Spark Session
spark = SparkSession.builder \
    .appName("Course_ML_Pipeline") \
    .getOrCreate()

# 2. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Î¿Ï… Î•Î½Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î¿Ï… Î‘Ï€Î¿Î¸ÎµÏ„Î·ÏÎ¯Î¿Ï… (Î’Î®Î¼Î± 4.2 & 4.3)
df = spark.read.option("multiLine", "true").json("unified_repository.json")

# 3. Î ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î±ÏƒÎ¯Î± Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ (Data Preprocessing - Î’Î®Î¼Î± 4.4)
tokenizer = Tokenizer(inputCol="summary", outputCol="words")
remover = StopWordsRemover(inputCol="words", outputCol="filtered")

# 4. Vectorization Î¼Îµ TF-IDF (Î’Î®Î¼Î± 4.4.1)
hashingTF = HashingTF(inputCol="filtered", outputCol="rawFeatures", numFeatures=1000)
idf = IDF(inputCol="rawFeatures", outputCol="features")

# 5. Clustering ÎœÎ±Î¸Î·Î¼Î¬Ï„Ï‰Î½ Î¼Îµ K-Means (Î’Î®Î¼Î± 4.4.2)
kmeans = KMeans(k=5, seed=1).setFeaturesCol("features").setPredictionCol("cluster_id")

# 6. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÎºÎ±Î¹ Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Pipeline
pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, kmeans])
model = pipeline.fit(df)
results = model.transform(df)

# 7. Export Î‘Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½ Î³Î¹Î± Ï„Î¿ API (Î’Î®Î¼Î± 4.4.3)
# Î•Ï€Î¹Î»Î­Î³Î¿Ï…Î¼Îµ Ï„Î± Î±Ï€Î±ÏÎ±Î¯Ï„Î·Ï„Î± Ï€ÎµÎ´Î¯Î±: Î¤Î¯Ï„Î»Î¿, Cluster ID ÎºÎ±Î¹ Ï„Î± TF-IDF Features (Î³Î¹Î± Similarity)
final_results = results.select("title", "source_course_id", "cluster_id", "features")

# Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÏƒÎµ JSON Î¼Î¿ÏÏ†Î® Ï€Î¿Ï… Î¸Î± Î´Î¹Î±Î²Î¬ÏƒÎµÎ¹ Ï„Î¿ API
final_results.write.mode("overwrite").json("ml_results.json")

print("ğŸš€ Î¤Î¿ Spark ML Pipeline Î¿Î»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")
spark.stop()
