import mysql.connector
import json
import csv
import os
import requests
import io # Î§ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ Î³Î¹Î± Î½Î± Î´Î¹Î¬Î²Î±ÏƒÎ¼Î± Ï„Î¿Ï… CSV Ï€Î¿Ï… Î­ÏÏ‡ÎµÏ„Î±Î¹ Î±Ï€ÏŒ Ï„Î¿ Î´Î¯ÎºÏ„Ï…Î¿
from datetime import datetime  # Î§ÏÎµÎ¹Î±Î¶ÏŒÎ¼Î±ÏƒÏ„Îµ Î±Ï…Ï„ÏŒ Î³Î¹Î± Ï„Î·Î½ Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î±

# --- Î¡Î¥Î˜ÎœÎ™Î£Î•Î™Î£ Î’Î‘Î£Î—Î£ ---
DB_CONFIG = {
    'user': 'root',
    'password': '',       
    'host': 'localhost',
    'database': 'spark',
    'port': 3306
}

# Î— Î»Î¯ÏƒÏ„Î± Ï€Î¿Ï… Î¸Î± Î¼Î±Î¶ÎµÏÎµÎ¹ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î³Î¹Î± Ï„Î¿ Spark
unified_data_for_spark = []

# --- URLS GITHUB (Î’Î‘Î£Î™Î£ÎœÎ•ÎÎ‘ Î£Î¤ÎŸ SCREENSHOT Î£ÎŸÎ¥) ---
# Î‘Ï…Ï„Î¬ ÎµÎ¯Î½Î±Î¹ Ï„Î± Raw Links Î±Ï€ÏŒ Ï„Î¿ repo ÏƒÎ¿Ï… 'dourougouu'
URL_UDACITY = "https://raw.githubusercontent.com/dourougouu/Project2-Spark-2025/main/FINAL/database/udacity_courses_j.json"
URL_COURSERA = "https://raw.githubusercontent.com/dourougouu/Project2-Spark-2025/main/FINAL/database/coursera_courses.csv"

# --- Î£Î¥ÎÎ‘Î¡Î¤Î—Î£Î•Î™Î£ Î’Î‘Î£Î—Î£ ---
def get_connection():
    return mysql.connector.connect(**DB_CONFIG)

def get_or_create_source(cursor, name, type_):
    cursor.execute("SELECT source_id FROM sources WHERE name = %s", (name,))
    res = cursor.fetchone()
    if res:
        return res[0]
    cursor.execute("INSERT INTO sources (name, type_) VALUES (%s, %s)", (name, type_))
    return cursor.lastrowid

def get_or_create_category(cursor, cat_name):
    if not cat_name: return None
    cat_name = cat_name.strip()[:140]
    cursor.execute("SELECT category_id FROM categories WHERE name_of_the_category = %s", (cat_name,))
    res = cursor.fetchone()
    if res:
        return res[0]
    try:
        cursor.execute("INSERT INTO categories (name_of_the_category) VALUES (%s)", (cat_name,))
        return cursor.lastrowid
    except mysql.connector.Error:
        return None

def upsert_course(cursor, source_id, source_course_id, title, summary, level, url, cats):
    cursor.execute("SELECT course_id FROM courses WHERE source_id=%s AND source_course_id=%s", 
                   (source_id, source_course_id))
    res = cursor.fetchone()
    
    if res:
        c_id = res[0]
        # Update (Î±Î½ Î¸Î­Î»Î¿Ï…Î¼Îµ Î½Î± ÎµÎ½Î·Î¼ÎµÏÏÎ½Î¿Ï…Î¼Îµ Ï€ÎµÏÎ¹Î³ÏÎ±Ï†Î­Ï‚ ÎºÏ„Î»)
        cursor.execute("""
            UPDATE courses SET title=%s, summary=%s, level_=%s, url=%s 
            WHERE course_id=%s
        """, (title, summary, level, url, c_id))
    else:
        # Insert
        cursor.execute("""
            INSERT INTO courses (source_id, source_course_id, title, summary, level_, url)
            VALUES (%s, %s, %s, %s, %s, %s)
        """, (source_id, source_course_id, title, summary, level, url))
        c_id = cursor.lastrowid

    # Categories Link
    if cats:
        for cat in cats:
            cat_id = get_or_create_category(cursor, cat)
            if cat_id:
                cursor.execute("""
                    INSERT IGNORE INTO course_categories (course_id, category_id) 
                    VALUES (%s, %s)
                """, (c_id, cat_id))

    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÏƒÏ„Î· Î»Î¯ÏƒÏ„Î± Î³Î¹Î± Ï„Î¿ ÎµÎ½Î¹Î±Î¯Î¿ JSON Ï„Î¿Ï… Spark
    unified_data_for_spark.append({
        "source_id": source_id,
        "source_course_id": str(source_course_id),
        "title": title,
        "summary": summary if summary else title,
        "level_": level,
        "url": url,
        "last_updated": datetime.now().strftime("%Y-%m-%d")
    })
    
    return c_id

# --- LOGIC Î“Î™Î‘ Î¤ÎŸ PATHING (Î¤ÎŸÎ Î™ÎšÎ‘ Î‘Î¡Î§Î•Î™Î‘) ---
def get_local_file_path(filename):
    # Î’ÏÎ¯ÏƒÎºÎµÎ¹ Ï€Î¿Ï ÎµÎ¯Î½Î±Î¹ Ï„Î¿ script (Î¼Î­ÏƒÎ± ÏƒÏ„Î¿ ml_spark)
    current_dir = os.path.dirname(os.path.abspath(__file__))
    # Î Î¬ÎµÎ¹ Î­Î½Î±Î½ Ï†Î¬ÎºÎµÎ»Î¿ Ï€Î¯ÏƒÏ‰ (..) ÎºÎ±Î¹ Î¼ÎµÏ„Î¬ ÏƒÏ„Î¿ database
    # Î¤ÎµÎ»Î¹ÎºÏŒ Path: .../FINAL/database/filename
    return os.path.join(current_dir, '..', 'database', filename)

# --- Î•Î Î•ÎÎ•Î¡Î“Î‘Î£Î™Î‘ UDACITY (JSON) ---
def process_udacity():
    print("\nğŸ” Processing Udacity...")
    data = None
    
    # 1. Î ÏÎ¿ÏƒÏ€Î¬Î¸ÎµÎ¹Î± Î±Ï€ÏŒ GITHUB (REST API Simulation)
    try:
        print(f"ğŸ“¡ Downloading from GitHub: {URL_UDACITY}")
        resp = requests.get(URL_UDACITY)
        if resp.status_code == 200:
            data = resp.json()
            print("Download success!")
        else:
            print(f"âš ï¸ GitHub returned {resp.status_code}")
    except Exception as e:
        print(f"Network error: {e}")

    # 2. Î‘Î½ Î±Ï€Î¿Ï„ÏÏ‡ÎµÎ¹, ÏˆÎ¬Ï‡Î½Î¿Ï…Î¼Îµ Î¤ÎŸÎ Î™ÎšÎ‘ ÏƒÏ„Î¿ ../database/
    if not data:
        local_path = get_local_file_path('udacity_courses_j.json')
        print(f"Falling back to local file: {local_path}")
        if os.path.exists(local_path):
            with open(local_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        else:
            print("File not found anywhere.")
            return

    # Î•Î¹ÏƒÎ±Î³Ï‰Î³Î® ÏƒÏ„Î· Î²Î¬ÏƒÎ·
    conn = get_connection()
    cursor = conn.cursor()
    sid = get_or_create_source(cursor, "Udacity", "json")
    
    # Î§ÎµÎ¹ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î´Î¿Î¼Î®Ï‚ JSON
    courses_list = data.get('courses', []) if isinstance(data, dict) else data
    
    count = 0
    for item in courses_list:
        title = item.get('Title') or item.get('title')
        if not title: continue
        
        summary = item.get('Summary') or item.get('summary') or ''
        level = item.get('Level') or item.get('level') or 'Unknown'
        url = item.get('Link') or item.get('search_url') or ''
        
        # ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÎµÏ€Î¹Ï€Î­Î´Î¿Ï… Î³Î¹Î± Î½Î± Ï„Î±Î¹ÏÎ¹Î¬Î¶ÎµÎ¹ ÏƒÏ„Î¿ ENUM Ï„Î·Ï‚ Î²Î¬ÏƒÎ·Ï‚
        if 'beginner' in level.lower(): level = 'Beginner'
        elif 'intermediate' in level.lower(): level = 'Intermediate'
        elif 'advanced' in level.lower(): level = 'Advanced'
        else: level = 'Unknown'

        # Categories (Î±Î½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Ï‰Ï‚ string "Business, Tech")
        cats_raw = item.get('affiliates') or '' 
        cats = [c.strip() for c in cats_raw.split(',') if c.strip()]

        upsert_course(cursor, sid, title[:200], title, summary, level, url, cats)
        count += 1
        
    conn.commit()
    conn.close()
    print(f"Udacity: Processed {count} courses.")

# --- Î•Î Î•ÎÎ•Î¡Î“Î‘Î£Î™Î‘ COURSERA (CSV) ---
def process_coursera():
    print("\n Processing Coursera...")
    csv_content = None
    
    # 1. Î ÏÎ¿ÏƒÏ€Î¬Î¸ÎµÎ¹Î± Î±Ï€ÏŒ GITHUB
    try:
        print(f"Downloading from GitHub: {URL_COURSERA}")
        resp = requests.get(URL_COURSERA)
        if resp.status_code == 200:
            # ÎœÎµÏ„Î±Ï„ÏÎ­Ï€Î¿Ï…Î¼Îµ Ï„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ ÏƒÎµ Î±ÏÏ‡ÎµÎ¯Î¿ ÏƒÏ„Î· Î¼Î½Î®Î¼Î· Î³Î¹Î± Ï„Î¿ CSV reader
            csv_content = io.StringIO(resp.text)
            print("Download success!")
    except Exception as e:
        print(f"Network error: {e}")

    # 2. Î‘Î½ Î±Ï€Î¿Ï„ÏÏ‡ÎµÎ¹, ÏˆÎ¬Ï‡Î½Î¿Ï…Î¼Îµ Î¤ÎŸÎ Î™ÎšÎ‘
    if not csv_content:
        local_path = get_local_file_path('coursera_courses.csv')
        print(f"Falling back to local file: {local_path}")
        if os.path.exists(local_path):
            csv_content = open(local_path, 'r', encoding='utf-8')
        else:
            print("File not found anywhere.")
            return

    # Î•Î¹ÏƒÎ±Î³Ï‰Î³Î® ÏƒÏ„Î· Î²Î¬ÏƒÎ·
    conn = get_connection()
    cursor = conn.cursor()
    sid = get_or_create_source(cursor, "Coursera", "csv")
    
    reader = csv.DictReader(csv_content)
    count = 0
    for row in reader:
        title = row.get('course_title')
        if not title: continue
        
        # Î¤Î¿ Coursera CSV Î´ÎµÎ½ Î­Ï‡ÎµÎ¹ unique ID, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î¿Î½ Ï„Î¯Ï„Î»Î¿
        c_id_str = title[:250] 
        summary = "" # Î¤Î¿ CSV ÏƒÎ¿Ï… Î¯ÏƒÏ‰Ï‚ Î´ÎµÎ½ Î­Ï‡ÎµÎ¹ summary, Ï„Î¿ Î±Ï†Î®Î½Î¿Ï…Î¼Îµ ÎºÎµÎ½ÏŒ Î® Î²Î¬Î¶Î¿Ï…Î¼Îµ Ï„Î¿Î½ Ï„Î¯Ï„Î»Î¿
        level = row.get('course_difficulty', 'Unknown')
        url = row.get('course_url', '')
        
        if 'beginner' in level.lower(): level = 'Beginner'
        elif 'intermediate' in level.lower(): level = 'Intermediate'
        elif 'advanced' in level.lower(): level = 'Advanced'
        else: level = 'Unknown'

        # Skills Ï‰Ï‚ categories
        skills = row.get('course_skills', '')
        cats = [c.strip() for c in skills.split(',') if c.strip()]

        upsert_course(cursor, sid, c_id_str, title, summary, level, url, cats)
        count += 1

    # Î‘Î½ Î±Î½Î¿Î¯Î¾Î±Î¼Îµ Ï„Î¿Ï€Î¹ÎºÏŒ Î±ÏÏ‡ÎµÎ¯Î¿, Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Ï„Î¿ ÎºÎ»ÎµÎ¯ÏƒÎ¿Ï…Î¼Îµ
    if isinstance(csv_content, io.IOBase) and not isinstance(csv_content, io.StringIO):
        csv_content.close()

    conn.commit()
    conn.close()
    print(f"Coursera: Processed {count} courses.")

if __name__ == "__main__":
    process_udacity()
    process_coursera()



#  Î•Î´Ï Î´Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯Ï„Î±Î¹  Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ JSON
    print("\n--- Î”Î—ÎœÎ™ÎŸÎ¥Î¡Î“Î™Î‘ Î•ÎÎ™Î‘Î™ÎŸÎ¥ Î‘Î ÎŸÎ˜Î•Î¤Î—Î¡Î™ÎŸÎ¥ (JSON) ---")
    with open('unified_repository.json', 'w', encoding='utf-8') as f:
        json.dump(unified_data_for_spark, f, ensure_ascii=False, indent=4)
    print(f"âœ… Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ 'unified_repository.json' Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®Î¸Î·ÎºÎµ Î¼Îµ {len(unified_data_for_spark)} ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚.")
        
    print("\n Harvesting completed!")

#    (oo)   (oo)   (oo)
#    /Â¥ \   /Â¥ \   /Â¥ \
#   _(__)_ _(__)_ _(__)_
#   HARVEST  HARVEST  HARVEST




